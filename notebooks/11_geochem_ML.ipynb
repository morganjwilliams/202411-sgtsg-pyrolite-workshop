{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1071dde8-bfa7-4484-ae27-3ffbc66b2a39",
   "metadata": {},
   "source": [
    "| [**Overview**](./00_overview.ipynb) | [Getting Started](./01_jupyter_python.ipynb) | **Examples:** | [Access](./02_accessing_indexing.ipynb) | [Transform](./03_transform.ipynb) | [Plotting](./04_simple_vis.ipynb) | [Norm-Spiders](./05_norm_spiders.ipynb) | [Minerals](./06_minerals.ipynb) | **Workflows:** | [lambdas](./07_lambdas.ipynb) | [CIPW](./08_CIPW_Norm.ipynb)  | [ML](./11_geochem_ML.ipynb) | [Spatial Data](./12_spatial_geochem.ipynb) |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96cd190-0e05-479b-89c0-5b51798cbb2c",
   "metadata": {},
   "source": [
    "# Towards some ML Applications\n",
    "\n",
    "This notebook focuses on using geochemistry for some machine learning applications. First we'll look at some mineral chemistry data to see how well we can classify titanium phase polymorphs and whether these are located with mineralization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71123424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyrolite\n",
    "\n",
    "np.random.seed(13)  # set a random seed, so everyone gets the same results.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e285947-86f5-4051-969f-a2583e407a12",
   "metadata": {},
   "source": [
    "## Rutile Geochemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34183db4-b1b9-46a0-92f0-334a785cb554",
   "metadata": {},
   "source": [
    "Included in this repository is a cleaned version of titanium phase minor + trace element abundances an appendix table from Plavsa et al. (2018), which we'll use here for an initial foray into using machine learning to help with some geological problems. This dataset was collected with addressing potential polymorph-based partitioning effects on mineralization indicators such that titanium phases across a broader range of rock types might be able to be compared. For more, see the paper:\n",
    "\n",
    "Plavsa, D., Reddy, S. M., Agangi, A., Clark, C., Kylander-Clark, A., & Tiddy, C. J. (2018). Microstructural, trace element and geochronological characterization of TiO2 polymorphs and implications for mineral exploration. Chemical Geology, 476, 130â€“149. https://doi.org/10.1016/j.chemgeo.2017.11.011\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e59d6b-65ab-4eba-9a01-646465dca139",
   "metadata": {},
   "source": [
    "On loading this paper, we can see that it is a dataset of reasonable but manageable size which consists of minor and trace elements (all in ppm), and columns for sample ID, phase ID and a binary indicator for whether the phase came from a mineralized system or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a8e12-d1d3-4237-b412-d4641f4cb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/rutile/Plavsa2018.csv\").set_index(\"Grain ID\", drop=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7f38f-53f4-4344-97ff-0d301f9efe16",
   "metadata": {},
   "source": [
    "### Some Minor Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bc8d9d-ebed-4343-808a-bb18dcf04d06",
   "metadata": {},
   "source": [
    "There are a few 'bdl' items for Hf within this table causing the column to show up as 'object' instead of a numerical type, which can complicate numerical analysis. For our purposes we can remove them by converting all the element data to numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e9810-44d6-402e-bdc4-12e0f07b9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3188f-3dd7-41ef-9d3a-7308dfedb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pyrochem.elements = df.pyrochem.elements.apply(\n",
    "    pd.to_numeric, errors=\"coerce\"\n",
    ").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35114f97-c488-40d5-94cb-b064e6891f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe26b96-41a2-4048-8204-13d20742321c",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ff65c-aa43-4464-a1e5-0b04bdcbd88d",
   "metadata": {},
   "source": [
    "To get an idea of how samples, phases and mineralization patterns might differ, we do a bit of exploratory data analysis, starting with some spider diagrams to see how patterns might look across all of these elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b82b6-8992-4bae-95af-9d78f59b2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrolite.plot.color import process_color\n",
    "from pyrolite.util.plot.legend import proxy_line\n",
    "\n",
    "\n",
    "def add_categorical_spider_legend(ax, categories, **kwargs):\n",
    "    ax.legend(\n",
    "        [\n",
    "            proxy_line(c=c, marker=\"D\", **kwargs)\n",
    "            for c in process_color(c=categories.unique())[\"c\"]\n",
    "        ],\n",
    "        categories.unique(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df472666-2f07-457e-89e5-dbe2f2e5f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 4))\n",
    "df.pyrochem.elements.pyroplot.spider(c=df[\"Sample ID\"], ax=ax, alpha=0.5)\n",
    "add_categorical_spider_legend(ax, df[\"Sample ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1a8ed-5fd8-4b02-9364-b0b796f68a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 4))\n",
    "ax = df.pyrochem.elements.pyroplot.spider(c=df[\"Phase ID\"], ax=ax, alpha=0.5)\n",
    "add_categorical_spider_legend(ax, df[\"Phase ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56033e6-1212-4135-8dad-9bfdc487a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 4))\n",
    "df.pyrochem.elements.pyroplot.spider(c=df[\"Mineralized\"], ax=ax, alpha=0.5)\n",
    "add_categorical_spider_legend(ax, df[\"Mineralized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282c81e-0a90-4487-ae3a-25020c07b2fb",
   "metadata": {},
   "source": [
    "## Putting some Numbers on the patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e337c3-4a7e-4616-bd74-30d102951401",
   "metadata": {},
   "source": [
    "We can clearly see a few key points early on - rutile is enriched in all minor elements other than Al relative to the other two phases which are otherwise trickier to pull apart (perhaps with Al, Fe). Mineralized and unmineralised titanium phases seem to show distinct patterns, especially for Sn, Cr, +/- Ta-W (take this with a grain of salt, noting we only have five individual samples in the dataset - it's still a useful first example). \n",
    "To get a more numerical overview of how these classes differ, we can use some of the inbuilt `pandas` functionality to get some overview statistics based on these breakdowns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13440d2f-30c9-436e-ac50-7dacdfef4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pyrochem.elements.groupby(df[\"Phase ID\"]).agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9331e52-5fb6-4a5b-9d4d-2d965f776286",
   "metadata": {},
   "source": [
    "We can add extra items in the chain (a log transform) and some styling to `pandas` output too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e82830-e4c2-4649-8002-6bde07389196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pyrochem.elements.apply(np.log).groupby(df[\"Phase ID\"]).agg(\n",
    "    \"mean\"\n",
    ").style.background_gradient(axis=0, cmap=\"cividis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7fb8a-513c-46e5-8502-d665dc48f846",
   "metadata": {},
   "source": [
    "We can also use multiple aggregation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1647d74-a9a6-4e06-8903-57fe4205e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pyrochem.elements.groupby(df.Mineralized).agg([\"mean\", \"std\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebe4ff-6fad-46ac-9413-e318f7abb2eb",
   "metadata": {},
   "source": [
    "## Building a Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c634bd4-6846-4c3f-a901-cdf41a8cbcee",
   "metadata": {},
   "source": [
    "With the quick detour into exploration complete for now, we can have a look at building some models which can i) automatically pick up on the patterns which are easy to see by eye above, ii) potentially pick up on some other subtleties and iii) be able to make predictions about new samples we might aquire down the line where we don't know the specific phase (e.g. hitting Ti hotspots in a grain mount under LAICPMS) or we don't know whether a system is mineralize (or, both!). The type of models we're working with below are generally referred to as predictive models, which are typically constructed more for making predictions than for inference/understanding the structure of a dataset.\n",
    "\n",
    "First, let's have a look at how well we can distinguish phases using the minor+trace element dataset. The key thing to notice, especially for these types of datasets, is the frequency of the most common label - this gives you a 'baseline' to compare to. Imagine if the model just predicted the most frequent phase all of the time - this would be the success rate (this is one reason imbalanced datasets can be hard to work with). Let's look a the proportions of phases in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36416d55-ac3d-4403-a9b0-91fad9291db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Phase ID\"].value_counts() / df.index.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a473a-dcd7-439d-b274-94c6820da0c7",
   "metadata": {},
   "source": [
    "So, at worst we should be able to score 45% without trying.\n",
    "\n",
    "In our predictive modeling problem we have a dataset which we want to use to predict another variable (or, target). This falls in the realm of *supervised learning*, which delineates structure in a dataset which relates to specific labels or values (e.g. classification, regression problems), as opposed to working from unlabeled data and attempting to create or delineate structure (e.g. clustering). In the simpler regression sense, this means we need an input `X` and a target `y`, and we're trying to construct a model `f` such that `f(X)` $\\approx$ `y`.\n",
    "\n",
    "In our case, the `X` is our elemental data, and the `y` or target our polymorph labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafe617-3af0-4f63-93ab-db8d9dd01df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.pyrochem.elements.apply(pd.to_numeric, errors=\"coerce\")\n",
    "y = df[\"Phase ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1fd0f-8e60-4f99-a9bf-a5f62af6325a",
   "metadata": {},
   "source": [
    "Note that the models below aren't necessarily going to deal with non numeric values (or NULL values) well, so we should make sure we get rid of them early:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590f5ea-a60e-4cb4-a0b9-e69d3ffd6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr = ~(pd.isnull(X).sum(axis=1) > 0)\n",
    "X, y = X.loc[fltr, :], y.loc[fltr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8b84e-3eed-4c08-8e89-3128d6c01704",
   "metadata": {},
   "source": [
    "We can see that we should still have 95% of our data, which is good to know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655afd4a-9578-485c-8bfb-0419978d1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr.sum() / fltr.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fdd980-99b7-408d-93f3-aec00f57b072",
   "metadata": {},
   "source": [
    "When it comes to predictive modeling using tabular data in Python, a key library worth getting acquainted with is [`scikit-learn`](https://scikit-learn.org/stable/index.html) (or `sklearn` as an import).\n",
    "\n",
    "Here we're going to import a specific classifer model class (there are many, check them out in the `scikit-learn` docs!), `sklearn.ensemble.RandomForestClassifier` which is a [random forest model](https://en.wikipedia.org/wiki/Random_forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db6ecb-d4d0-453d-a509-f39fced81bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e632d4-070f-47d2-bfe4-041a3d32c22b",
   "metadata": {},
   "source": [
    "In order for everyone to get the same results, we can set the random state, which governs non-deterministic behaviour (random effects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07f15e-ecee-4e09-8234-52623dcf8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 27\n",
    "classifier = RandomForestClassifier(random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fe2f1-4fbc-4026-a47e-98b9c565995a",
   "metadata": {},
   "source": [
    "First we want to fit our classifier model to our input data and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1368cb-37af-4eca-963a-e6f7f2ef67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bccd76-3f6c-4be4-b613-59166ef4b860",
   "metadata": {},
   "source": [
    "Let's make some predictions on our dataset and see how well we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551758d-f60e-45ee-b8fb-9e84f1adda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1a249-034d-46c3-9c9c-dd14b92923cc",
   "metadata": {},
   "source": [
    "We can check these against what we know the phases to be by sticking them both in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1668816-b8b7-4ce9-975a-079d05746f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[fltr, [\"Phase ID\"]].assign(Predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bbcd52-8f79-4969-822c-97501d0944b9",
   "metadata": {},
   "source": [
    "Or simply calling score on the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b65fb7-72eb-4615-b179-65ad55c276bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8484e8e-9cc8-4b81-884f-9086fbab8d31",
   "metadata": {},
   "source": [
    "It looks pretty good, but this wasn't really a fair test - we're using the same data to train the model as we are to examine it; this is referred to as 'data leakage' in ML modeling. Instead what we want to do is keep back a hold-out-set for testing the model which we don't use for training the model. `scikit-learn` has some built in tools for this, thankfully. These allow us to specify the proportion of data we keep for testing, and whether we 'stratify' the dataset such that we have roughly equal proportions of labels in our training and testing sets (in this case, that's probably a good idea):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fce71-2b31-4a0e-8f9e-e5aafba272e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, XX_test, yy_train, yy_test = train_test_split(X, y, stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3103a5-fb21-4d18-8b47-577cf0f641fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train.index.size, XX_test.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb22b59-16e0-4e0d-8ff3-2d91b1025a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=random_state).fit(XX_train, yy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406219fa-74b6-4097-b16a-9d83cd103e26",
   "metadata": {},
   "source": [
    "We don't do quite as well here, but this is closer to what it might look like in the real world (assuming the things we find are similar to those in our training set, of course..):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673facaf-cdf9-49a2-9d92-f196a4c1a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(XX_test, yy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413c3f1-f763-4371-9c54-a6d405015022",
   "metadata": {},
   "source": [
    "We can also chain `scikit-learn` components together to make a 'pipeline', adding additional preprocessing steps or bringing together/splitting parts of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94241776-6279-48be-9cac-62a55f168eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pyrolite.util.skl.transform import LogTransform\n",
    "\n",
    "transform = LogTransform()  # log-transform the data\n",
    "clf = RandomForestClassifier(random_state=random_state)  # our classifer model\n",
    "pipe = make_pipeline(transform, clf)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52940f2b-d286-40b2-b42b-6014a4cfd5c8",
   "metadata": {},
   "source": [
    "In this case it won't help us much (scaling isn't necessarily needed for random forests, and we have a fairly high overall classification accuracy), but it's good to keep in mind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731d3ae-adf7-40f2-95c0-e172e28e3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(XX_train, yy_train).score(XX_test, yy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec358aef-8f5e-48da-9b69-24327d8f474b",
   "metadata": {},
   "source": [
    "Another way to look at how well a classifier model performs is on a per-class basis, such as that used in confusion matrix. In this instance is shows us some new information we wouldn't necessarily have seen otherwise - that our predictions for brookite are the worst, and it's mostly because it's getting misclassified as anatase (~4% of the time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45591307-f6f4-4901-878b-133c452bb576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrolite.util.skl.vis import plot_confusion_matrix\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(5, 4))\n",
    "\n",
    "plot_confusion_matrix(pipe, XX_test, yy_test, ax=ax, normalize=True)\n",
    "\n",
    "ax.set_title(\"Polymorph Classifier\\nConfusion Matrix\")\n",
    "ax.set(xlabel=\"Predicted Polymorph\", ylabel=\"True Polymorph\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cae7e3-8540-4a6e-a6f9-008236a6094f",
   "metadata": {},
   "source": [
    "## Mineralisation Classifier\n",
    "\n",
    "We can do the same for mineralization which we've done for polymorphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55626df-ab74-46b4-8ce7-5a1dc5c33eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.pyrochem.elements.apply(pd.to_numeric, errors=\"coerce\")\n",
    "y = df[\"Mineralized\"]\n",
    "\n",
    "fltr = ~(pd.isnull(X).sum(axis=1) > 0)\n",
    "X, y = X.loc[fltr, :], y.loc[fltr]\n",
    "\n",
    "XX_train, XX_test, yy_train, yy_test = train_test_split(X, y, stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bf395-4eb6-4007-b499-78d190bb9566",
   "metadata": {},
   "source": [
    "Our baseline here is 71%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a19e6-5027-463d-8bd0-850bfe4e50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts() / y.index.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799a228-baef-4386-9fdd-3b1f1bf7f348",
   "metadata": {},
   "source": [
    "Which we can easily surpass uisng the same pipeline we created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecae486-4221-4aa6-87da-7997630b6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(XX_train, yy_train).score(XX_test, yy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba95f3-f2a9-4215-a315-74f8160edc4f",
   "metadata": {},
   "source": [
    "Given we're working with random forests, a handy thing to do at this stage might be to look at the relative feature importances, which can be accessed from the 'model'/estimator part of the pipeline, which happens to be the last bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fa837-5fa6-4aab-b2c1-1e5bba7b30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.steps[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c428c84-61f8-4908-b2cf-fb373e921480",
   "metadata": {},
   "source": [
    "As might be expected, Sn tops the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947925c-2aee-428a-a617-017228d3af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pipe.steps[-1][1].feature_importances_, index=X.columns).sort_values(\n",
    "    ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb772c-69db-439c-b272-222736e49119",
   "metadata": {},
   "source": [
    "This is where we'll leave this example - there's likely more we could do to make it peform in the real world, but for the purpose here it looks pretty good! Given the initial intention of the paper, what could be done is constructing a model to identify polymorphs, which is then fed in as an additional input to a mineralization classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0af14-a5a2-4928-8678-3e7cd7ba50e9",
   "metadata": {},
   "source": [
    "Often the best place to start in improving models is cleaning up your data (it's rare that you'll start with super-clean data). To go beyond this, I'd suggest first having a look at playing with different models, then hyperparameter optimization - changing the internal parameters of our workflow components to optimally achieve our desired objective. `scikit-learn` has some built in tools for this too, of course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0d7e6-b687-4ed5-8d6c-f8fc797eeece",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "| [**Overview**](./00_overview.ipynb) | [Getting Started](./01_jupyter_python.ipynb) | **Examples:** | [Access](./02_accessing_indexing.ipynb) | [Transform](./03_transform.ipynb) | [Plotting](./04_simple_vis.ipynb) | [Norm-Spiders](./05_norm_spiders.ipynb) | [Minerals](./06_minerals.ipynb) | **Workflows:** | [lambdas](./07_lambdas.ipynb) | [CIPW](./08_CIPW_Norm.ipynb)  | [ML](./11_geochem_ML.ipynb) | [Spatial Data](./12_spatial_geochem.ipynb) |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
